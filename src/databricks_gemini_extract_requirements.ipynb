{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - Gemini Extract Requirements - Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e60624e5-a914-42a1-a55c-4d37dd9de86f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc9b0da5-bf14-48e0-824d-33515cefbb2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5158573e-6368-4c74-b863-9406f0a857c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from google.api_core import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1cc241-d5e9-47d0-b6e6-c4331045474b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import rapidfuzz.utils\n",
    "import thefuzz\n",
    "from thefuzz import process, utils\n",
    "from copy import copy\n",
    "import ast\n",
    "from rapidfuzz import fuzz as rapidfuzz\n",
    "\n",
    "\n",
    "VALUE_ERROR = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9498669-bb7b-431a-b669-90f0c6172ed6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08388721-49d2-4942-af44-79849025505b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install thefuzz\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c214f7c-655a-4d40-b777-029cde570d49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Gemini Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f4fc8e-b093-4b31-9800-cb95990fd243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = my_api_key\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d17f45e-9182-4f3c-9356-8a80d41ca6f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "edu_item = glm.Schema(\n",
    "    type = glm.Type.OBJECT,\n",
    "    properties = {\n",
    "        'type_of_education':  glm.Schema(type=glm.Type.STRING),\n",
    "        'education_in_field':  glm.Schema(type=glm.Type.STRING),\n",
    "    },\n",
    "    required=['type_of_education', 'education_in_field']\n",
    ")\n",
    "exp_item = glm.Schema(\n",
    "    type = glm.Type.OBJECT,\n",
    "    properties = {\n",
    "        'experience_in_field':  glm.Schema(type=glm.Type.STRING),\n",
    "        'minimal_years_of_experience_in_the_field': glm.Schema(type=glm.Type.STRING)\n",
    "    },\n",
    "    required=['experience_in_field', 'minimal_years_of_experience_in_the_field']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96232c83-fd18-4d26-b4f6-86e5fff939c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "edu_reqs = glm.Schema(\n",
    "    type=glm.Type.ARRAY,\n",
    "    items=edu_item\n",
    ")\n",
    "exp_reqs = glm.Schema(\n",
    "    type=glm.Type.ARRAY,\n",
    "    items=exp_item\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb673bb-7a43-4be4-a807-2a952adc26d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "extract_requirements = glm.FunctionDeclaration(\n",
    "    name=\"extract_requirements\",\n",
    "    description=textwrap.dedent(\"\"\"\\\n",
    "        extract required education items and required experience items from the job posting\n",
    "        \"\"\"),\n",
    "    parameters=glm.Schema(\n",
    "        type=glm.Type.OBJECT,\n",
    "        properties = {\n",
    "            'required_education_item_array': edu_reqs,\n",
    "            'required_experience_item_array': exp_reqs,\n",
    "            },\n",
    "        required=['required_education_item_array', 'required_experience_item_array']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3718787-9b75-489a-9bfe-ec3c5e92cb8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = model = genai.GenerativeModel(\n",
    "    model_name='gemini-1.0-pro-latest',\n",
    "    tools = [extract_requirements]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f73e30f-3a46-425c-b8b1-ef12b182acae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Please extract education and experience requirements from the following job posting description following these guidelines:\n",
    "required_education_item_array is an array of the education requirement items that appear in the job posting description.\n",
    "all required education mentioned should be extracted and each should appear seperatly in an item containing the following two properties:\n",
    "    1. type_of_education - string of the type of education. guidelines specific to this type are:\n",
    "        a. for any non academic education requirement use \\\"Non-Academic\\\", including high school\n",
    "        b. for academic degrees use abbreviations when possible (e.g. \\\"BS\\\" instead of \\\"Bachelor of Science\\\")\n",
    "        c. for certificate programmes use \\\"Certification\\\"\n",
    "        d. for diplomas use \\\"Diploma\\\" (not high school diplomas)\n",
    "        e. in any other case or when not sure, extract type \\\"Other\\\"\n",
    "    2. education_in_field - string of the field of education related to type_of_education mentioned above. guidelines specific to this type are:\n",
    "        a. omit the type from this field as it should be exracted seperatly to the previous field of this item.\n",
    "        b. use the most general and consice term for the topic of study (e.g. \\\"Psychology\\\" instead of \\\"Behavioral and Clinical Psychological Analysis\\\")\n",
    "required_experience_item_array is an array of the experience requirement items that appear in the job posting description.\n",
    "all required experience mentioned should be extracted and each should appear seperatly in an item containing the following two properties:\n",
    "    1. experience_in_field - string of the field title or previous position title in which the experience requirement is. guidelines specific to this type are:\n",
    "        a. use the most general and consice term for the field\n",
    "        b. if no field or past position is mentioned, extract \\\"Any\\\"\n",
    "    2. minimal_years_of_experience_in_the_field - string of float of the number of years required in the field mentioned above. guidelines specific to this type are:\n",
    "        a. if time ranges of experience in field are mentioned, extract the lower number (e.g. \\\"5 to 10 years of experience\\\" should be \\\"5.0\\\", \\\"7+ years\\\" should be \\\"7.0\\\")\n",
    "        b. if number of months is mentioned like in \\\"4 years and 3 months required\\\", write it as \\\"4.25\\\".\n",
    "        c. if no time related to the experience field is mentioned, write null here.\n",
    "\n",
    "the job posting description:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22f4107e-c8db-4b6a-8696-d93a445e8e68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Job Posting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568a2a2c-f664-495b-90c9-d5ac62a621d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dir_path = \"dbfs:/FileStore/tables/\"\n",
    "file_name = \"job_skills_part_1.parquet\"\n",
    "file_path = dir_path + file_name\n",
    "df1 = spark.read.parquet(file_path, header = True)\n",
    "df2 = spark.read.parquet(\"dbfs:/FileStore/job_skills_part_2.parquet\", header = True)\n",
    "df3 = spark.read.parquet(\"dbfs:/FileStore/job_skills_part_3.parquet\", header = True)\n",
    "df4 = spark.read.parquet(\"dbfs:/FileStore/job_skills_part_4.parquet\", header = True)\n",
    "df5 = spark.read.parquet(\"dbfs:/FileStore/job_skills_part_5.parquet\", header = True)\n",
    "df6 = spark.read.parquet(\"dbfs:/FileStore/job_skills_part_6.parquet\", header = True)\n",
    "\n",
    "df = df1.union(df4).union(df3).union(df2).union(df5).union(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ef0a7d-38c0-4ce0-be46-d22b26b25ea1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: 1296381"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ac565a5-93a2-4f48-bbf4-c179be4ad261",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Job Posting position column to chosen positions conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5e6f49-cef3-43a2-a034-fd6976a11c60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: ['Data Manager',\n",
      " 'Clinical Coordinator',\n",
      " 'Division Manager',\n",
      " 'Logistics Analyst',\n",
      " 'Building Manager']"
     ]
    }
   ],
   "source": [
    "choices_new = spark.read.parquet(\"dbfs:/user_data/g37/choices_new.parquet\")\n",
    "choices = list(choices_new.toPandas().to_dict()['0'].values())\n",
    "choices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d12fc91-0952-4246-b0e5-f251637627ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_match(title):\n",
    "    matches = process.extractOne(title, choices, scorer=rapidfuzz.token_set_ratio,\n",
    "                                 processor=thefuzz.process.default_processor)\n",
    "    if matches[1] < 90:\n",
    "        return None\n",
    "    return matches[0]\n",
    "get_match_udf = F.udf(get_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1370942-4105-4fe6-83b4-4405496aa006",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: 442646"
     ]
    }
   ],
   "source": [
    "dfsk = df.select(\"*\")\n",
    "dfsk = dfsk.withColumn('narrow_position', get_match_udf(F.col('position')))\n",
    "dfsk = dfsk.dropna(subset=['narrow_position'])\n",
    "dfsk.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c35a68d-658c-4193-8d9f-6c44e3c6a30a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run Gemini On Job Positions And Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ec1588-9c6f-4f04-8224-e6424abd5733",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def convert_summary(summary):\n",
    "    description = prompt + summary\n",
    "    success = False\n",
    "    max_tries = 1\n",
    "    while not success and max_tries > 0:\n",
    "        try:\n",
    "            res = model.generate_content(description)\n",
    "            # res = 0\n",
    "            success = True\n",
    "        except:\n",
    "            success = False\n",
    "            max_tries -= 1\n",
    "    if max_tries == 0:\n",
    "        return None\n",
    "    try:\n",
    "        fc = res.candidates[0].content.parts[0].function_call\n",
    "        jfc = json.dumps(type(fc).to_dict(fc), indent=4)\n",
    "        res_dict = json.loads(jfc)\n",
    "        return str({k: v if v is not None else [] for k, v in res_dict[\"args\"].items()})\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b509cb39-e999-4a7b-a42a-514cb7a1a02e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 24\n",
    "sample_size = 1000\n",
    "dfsk_len = dfsk.count()\n",
    "\n",
    "for i in range(n_samples):\n",
    "    sdf = dfsk.sample(fraction=sample_size/dfsk_len).toPandas()\n",
    "    sdf['job_summary_processed'] = sdf['job_summary'].apply(convert_summary)\n",
    "    spark.createDataFrame(sdf).write.parquet(f\"dbfs:/user_data/g37/skills_true_narrowed_{sample_size}_{i}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_gemini_extract_requirements",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
